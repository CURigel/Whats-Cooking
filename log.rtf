{\rtf1\ansi\ansicpg1252\deff0\nouicompat{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset2 Symbol;}}
{\*\generator Riched20 10.0.10586}\viewkind4\uc1 
\pard\sa200\sl240\slmult1\b\f0\fs28\lang9 What's Cooking Log - Devin Denis \fs22\par

\pard\sa200\sl276\slmult1 Introduction\par
\b0\tab This document exists to organize and record my thoughts as I try out some things on my first Kaggle competition: What's cooking.  If you're reading this and you aren't me, expect it to be pretty informal, as it's mostly just so I don't forget things and can look back later to see why I made the decisions I did.\par
\b 8/12/2015\fs28\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0\fs22 After some deliberation on which dataset to try out, downloaded the what's cooking dataset.  I like cooking, this should be interesting.\par
{\pntext\f1\'B7\tab}First thought:  I feel like it'd be fairly straightforward to come up with a set of rules by hand that would cover most recipies, with just a little research.  But I guess the point of this is to have the computer do it for you.  And maybe it's harder than I think.\par
{\pntext\f1\'B7\tab}Okay, so the dataset is in JSON.  haven't worked with that before, but there's probably something in pandas to read it it.\par
{\pntext\f1\'B7\tab}Yep, there is.  So I get three columns: id, cuisine, and a list of ingredients.  The list isn't going to be very useful as is (at least, as far as I know), so I'll have to do some transforming.  My immediate thought is to have one column for every ingredient, and a row has a 1 if that ingredient appears in the recipe.  That would be a lot of columns and possibly really inefficient, but I think I'll go ahead and try it.  See how it goes.\par
{\pntext\f1\'B7\tab}Okay, so I wrote a couple of simple for loops to find all the unique ingredients and then create a column for each.  It takes a few minutes to run on the ~40k recipies.  I'm going to try saving the resulting matrix, so I can just read it in later instead of doing the manipulation every time.\par
{\pntext\f1\'B7\tab}Saving isn't going to work.  After some fiddling and wrong steps, I got it to actually save.  It takes about 20 minutes.  I tried loading it and that also took a long time.  Long enough that I gave up.  There's definitely got to be a better way of doing this.\par
{\pntext\f1\'B7\tab}So the meat of my matrix is 6700 some columns where everything is 0-1.  I've been taking a string processing class and we've done a bunch of bitparallel algorithms, which is making me think of representing the rows with bit vectors.  That would reduce the size by a pretty large factor.  But then I have to do all the matrix operations on bit vector matricies.  I feel like I'm very likely not the first person to think of doing something like this, and since all the entries are 0-1 it may be quite possible to do all the matrix stuff with bitparallel operations.\par
{\pntext\f1\'B7\tab}After some googling, I haven't found anything about bitparallel matrices.  And implementing this stuff my self is kind of getting away from the real point of this excersize, so I think I'll leave this thread for now\par
{\pntext\f1\'B7\tab}So I looked up sparse matricies, because I'd heard the term a few times before, and since most of the columns in any row are zero, my matrix seems like it's pretty "sparse".  And indeed it fits the definition of a sparse matrix very well.  I found some scipy documentation on their sparse matrix representations, so I think I'll give those a look tomorrow.\par

\pard\sa200\sl276\slmult1\b 9/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 So sparse matricies.  I found some code showing that sklearn can handle them.  Which kind of makes sense.  As long as they implement their own operations it should be pretty seamless.  The sklearn code was using lasso, which I have only vaugely heard of before (my understanding is it's a scoring function for linear regression, but I could be wrong).  I think I'll try it first, the sklearn documentation on it says it's for sparse data.  And it should be easy enough to swap it out for something else later if I want.\par
{\pntext\f1\'B7\tab}So there are a bunch of different sparse matrix representations.  After reading the documentation on some of them, I think the best way to go for now is coo to build and csc to do the work.  That's what the sklearn example used.\par
{\pntext\f1\'B7\tab}Also just realized- I should put this on github.  I'm not sure I actually want to show it to anyone, but if I decide to later, putting it up now will look better, cause there'll be commit messages and so on.\par
{\pntext\f1\'B7\tab}Also what is this .idea folder that came with the kaggle dataset?  I should look that up at some point.\par
{\pntext\f1\'B7\tab}Having some difficulty actually getting the sparse matrices to build and save.\par

\pard\sa200\sl276\slmult1\b 13/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Scipy's mmwrite won't save bools.  Interesting.  I probably would've found that in the docs if I'd read them before using.\par
{\pntext\f1\'B7\tab}I'm not sure what a lot of these Lasso parameters mean.  Fit_intercept should be false if the data is already centred.  Well the data is 0-1.  Is that centred?  I'm going to put true, because it seems like if that's wrong, it'll cost efficiency, not correctness.\par
{\pntext\f1\'B7\tab}So this is a multiclass problem, and lasso is a form of linear regression, which is for binary classification.  In class we were taught we could deal with this in two ways- one vs all, or all vs all.  There are probably sklearn built-ins to do this, I'll do some googling.\par
{\pntext\f1\'B7\tab}So after googling, looks like there is a good support for multiclass of various types in general, but I may have to use logistic regression instead of lasso?  Maybe lasso is a subclass. \par
{\pntext\f1\'B7\tab}Theoretically this should work fine with lasso, but for simplicity's sake I'll go with logistic regression for now.  It apparently handles sparse input fine.\par

\pard\sa200\sl276\slmult1\b 14/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 So I finally took a look at that .idea folder- it's not from kaggle, it's pycharm's project settings.  I set git to ignore the workspace.xml part and keep committing the rest.\par
{\pntext\f1\'B7\tab}I've mapped cuisines to ints and added a basic sklearn setup for logisitic regression.  It's not working.  It seems to predict the same cuisine for every item (I did some step throughs of some parts of the code and y_predict was all the same number).\par
{\pntext\f1\'B7\tab}So looking at the internals of the logistic regression object after calling fit, it looks like the weight vectors are all zeros, but the intercepts are actual numbers.  I think I'll try telling it not to fit intercepts and see if that does anything useful.\par
{\pntext\f1\'B7\tab}Perhaps predicatably, that results in intercept 0 but coefficients also zero.  It's definitely time to step back and do more research, hopefully to properly understand what's going on with logistic regression and what the various parameters do.\par
{\pntext\f1\'B7\tab}So apparently somewhere along the way from loading in the json to loading the saved csr matrix, every entry ends up as zero.  This explains the logistic regression alg not working.  At least I have a direction to investigate now.\par

\pard\sa200\sl276\slmult1\b 29/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Took a break for Christmas.  Very glad I kept a log of what I was doing.  Should definitely make this a habit.\par
{\pntext\f1\'B7\tab}So it's not the loading that's the problem.  The column calcultion is giving zeros everywhere.  I thought I made sure this was working before I moved on, but apparently I made a mistake.  Specifically, the line\line whats_cooking["ingredients"] == ingredient\line does not search the list in the ingredients column for the value of ingredient, which for some reason I believed it did.  Looking at alternatives now.\par
{\pntext\f1\'B7\tab}So doing this efficiently might be a bit more difficult than I originally imagined.  I could brute force it, but god that'll be so slow with a big table like this.  Found a stackoverflow question which gives a vectorized way, but it's a bit complicated.  Gonna break it into steps and print each bit and try and see if a) it works b) I can figure out why it works\par
{\pntext\f1\'B7\tab}So apply.(pd.series) on the column of lists spreads them out into columns, which is cool and potentially useful elsewhere as well.  Stack turns that into one column with an inner level that consists of all the rows.  Get dummies does what it says on the tin, but there's still two levels of index.  And the sum makes that one level.  I'm pretty clear \i what\i0  each step does, but still a bit unclear on the \i how\i0  part for the sum bit.\par
{\pntext\f1\'B7\tab}So that seems to have worked, but also the maximum value is 2 and not one.  Maybe an ingredient was listed twice in some recipe? \par
{\pntext\f1\'B7\tab}Found the recipies that had values of 2: (crumbled blue cheese, salt, large egg whites, vegetable oil, sugar, cooking spray, all-purpose flour, large eggs, 2%reduced-fat milk) and (2% reduced-fat milk, corn starch, eggs, maple syrup, sugar, margarine, vanilla extract).  No repeat ingredients (except maybe 'large egg whites' overlapping with 'large eggs' in the second, but there's no reason to expect non-exact matches).  There's the % sign in the 2% milk, but they're string objects, that really really shouldn't be causing any problems.  Not sure what's going on.  Which worries me.  \par
{\pntext\f1\'B7\tab}Running logisitic regression on 5000 entries gives me a ~50% correct prediction rate averaged over 3 cross validation folds.  Which is pretty exciting!  Actually better than random prediction!  Still a bit worried about the columns which have a value of 2, but I'm going to put a pin in that and come back to investigate later.\par
{\pntext\f1\'B7\tab}Also, it still works when doing a load instead of creating columns normally.  Which is excellent!\par

\pard\sa200\sl276\slmult1\b 2/1/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 So running on the full data set with 10 cross validation folds gives me ~70% correct prediction.  Not entirely terrible, but I'm pretty sure I can do better.  First thing's first- I'm going to look into the issue with some cells getting values other than 0 or 1.  Hopefully I'll be able to figure out what's going on there.  And even if I can't, it's still likely a good idea to reduce all the 2s to 1s.\par
{\pntext\f1\'B7\tab}I may have found  the wrong recipies the last time.  I had been using argmax() and doing some math to calculate the relevant rows.  This time I did "(ing_col == 2).any(axis=1)" and used that as a row index.    Maybe I made a math error last time?  In particular, I think this is right because it gives a much more sensible answer.  I find only one row that has 2 valued cell(s), and it does have duplicate ingredients.  Going to check if I get similar results on a bigger portion of the data, and if I do, I'll just set everything greater than 1 to 1.\par
{\pntext\f1\'B7\tab}Also, apparently the matrix classes in numpy are sort of deprecated in favour of ndarrays?  I can see how having both is kind of annoying.  I need the sparse matrices, but I'll favor ndarrays over the regular ones.\par
{\pntext\f1\'B7\tab}So searching the whole data set, there are five recipies that show up as having values other than 0 or 1, and they all have duplicate ingredients.  So that answers that question.  As I said before, I'll just set everything above 1 to 1.\par
{\pntext\f1\'B7\tab}Unsurprisingly, correcting the values for the five recipies that had duplicates didn't really change the accuracy significantly.  Still at ~70%.\par
{\pntext\f1\'B7\tab}So, I definitely want to try a decision tree based approach here.  We've got enough data to mitigate some of the overfitting issues, and I think it makes sense with the boolean data.  Also, I feel like distinguishing recipies not just ingredient by ingredient but  by groups of ingredients together might be more effective.  For example, maybe ginger = pretty much anything asian, garlic = anything eurasian, but ginger + garlic together = specifically chinese.  (Lack of linear separability?)\par
{\pntext\f1\'B7\tab}Going to start with a random forest.  Going to set the max depth to 5 (how many recipies have more than 5 distinguishing ingredients?)  I'll start a small number of trees looking at the standard sqrt(num_features) things in case time to train is too much, and work the number up over a few runs if I can.\par
{\pntext\f1\'B7\tab}So with 10 trees I get ~33% accuracy.  Pretty bad, but that's only 10 trees.  Also, it occurs to me that filtering out the ingredients that only appear in a few recipies would very likely make this work better, as the chance of a tree getting a relevant ingredient selected randomly would be higher.  I think I'll go ahead and ignore ingredients that appear in fewer than 10 recipies.  \par
{\pntext\f1\'B7\tab}Also, I haven't really been worrying about ingredients that are basically the same appearing multiple times in very slightly different forms.  It might be interesting to try an approach later where I get a list of canonical important ingredients (maybe there's some research out there about what ingredients are important to separate cuisines), and look for whether a given recipe has something that resembles that in it.  Still, that's for later, not now.\par
{\pntext\f1\'B7\tab}Interestingly, after filtering but still at 10 trees, accuracy went down to ~20%.  Maybe 'must be in at least 10 recipies' was too strict of a criteria.  Aaaaaand going up to 100 basically didn't change the value.  Trying 100 trees on the unfiltered data also gives pretty bad results (~34%).  Maybe I'll try taking out my max depth parameter?  The default value is unlimited (nevermind, that runs for an unresonable amount of time).  Going up to 20 depth and 50 trees increased accuracy to ~52%.  Better, but still much worse than the linear classifier.  \par

\pard\sa200\sl276\slmult1\par
\b 3/1/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 I think today I'll mostly be playing around with the random forest meta parameters and trying to get a decent result (without making my computer catch fire).  I also want to try filtering using less strict criteria, playing with the logisitic regression parameters, using filtering + logistic regression, and (if I can get a decent result with the trees) ensembling the logisitic regression and random forest\par
{\pntext\f1\'B7\tab}Hmm.  Almost the exact same result for random_forest with 50 trees and max depth 20 on data filtered with "must be in at least 3 recipies" as I had on the stricter filtering and worse trees.  Definitely something wrong there.\par
{\pntext\f1\'B7\tab}Something just occurred to me that I probably should have considered sooner.  The test set may have a lot of ingredients that don't appear in any of the recipes in the training set and vice versa.  I think that's definitely a mark in favor of filtering out ingredients that aren't repeated in many recipies, because fitting to those is likely going to be very bad for performance on the training set.  Speaking of the training set, I should bring that out and try the algorithm on it at the end of today, whatever I've got.\par
{\pntext\f1\'B7\tab}Another sudden 'uh oh'.  Does random forest support sparse matricies?  Maybe not.  Although if it completely doesn't support it getting the 50% accuracy earlier might not make any sense.  \par
{\pntext\f1\'B7\tab}Documentation says it can take sparse matricies, so that should be fine then.  I might do a run on the dense version just to make sure I'm not missing anything, but it should work the same.\par
{\pntext\f1\'B7\tab}I can't find anything wrong with the filtering, inspecting the process.  It very much seems correct.  But the results are absolutely terrible.  Maybe I'll try upping the number of trees, the fitting doesn't seem to take very long. \par
{\pntext\f1\'B7\tab}Maybe on the unfiltered set, the trees are looking for specific ingredients that only appear in one or two recipies, and finding one of those for each recipe in a cuisine, and then building a 'do you have any of these specific ingredients' type of tree.  Instead of looking at what more common ingredients can be used to distinguish.  Looking at the more common ingredients is probably a much harder problem, (needing more trees), but it'll generalize better.  And the reduced dimensionality (~6500 to ~2500 just filtering out things that appear in 2 or less recipies, wow) should allow more trees to be trained in the same amount of time.\par

\pard\sa200\sl276\slmult1\b 7/1/2016\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Wait, shouldn't cross validation prevent the issue I described in my last entry?  Also, woops, forgot to commit before several days of laziness.\par
{\pntext\f1\'B7\tab}So I want to figure out what's going wrong with my filtering stuff.  Both random forest and logistic regression perform extremely poorly on the filtered set.  Logisitic has a 2% correct prediction rate, which seems unlikely to happen unless something is screwed up during the filtering.\par

\pard\sa200\sl276\slmult1\par
}
 