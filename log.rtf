{\rtf1\ansi\ansicpg1252\deff0\nouicompat{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset2 Symbol;}}
{\*\generator Riched20 10.0.10586}\viewkind4\uc1 
\pard\sa200\sl240\slmult1\b\f0\fs28\lang9 What's Cooking Log - Devin Denis \fs22\par

\pard\sa200\sl276\slmult1 Introduction\par
\b0\tab This document exists to organize and record my thoughts as I try out some things on my first Kaggle competition: What's cooking.  If you're reading this and you aren't me, expect it to be pretty informal, as it's mostly just so I don't forget things and can look back later to see why I made the decisions I did.\par
\b 8/12/2015\fs28\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0\fs22 After some deliberation on which dataset to try out, downloaded the what's cooking dataset.  I like cooking, this should be interesting.\par
{\pntext\f1\'B7\tab}First thought:  I feel like it'd be fairly straightforward to come up with a set of rules by hand that would cover most recipies, with just a little research.  But I guess the point of this is to have the computer do it for you.  And maybe it's harder than I think.\par
{\pntext\f1\'B7\tab}Okay, so the dataset is in JSON.  haven't worked with that before, but there's probably something in pandas to read it it.\par
{\pntext\f1\'B7\tab}Yep, there is.  So I get three columns: id, cuisine, and a list of ingredients.  The list isn't going to be very useful as is (at least, as far as I know), so I'll have to do some transforming.  My immediate thought is to have one column for every ingredient, and a row has a 1 if that ingredient appears in the recipe.  That would be a lot of columns and possibly really inefficient, but I think I'll go ahead and try it.  See how it goes.\par
{\pntext\f1\'B7\tab}Okay, so I wrote a couple of simple for loops to find all the unique ingredients and then create a column for each.  It takes a few minutes to run on the ~40k recipies.  I'm going to try saving the resulting matrix, so I can just read it in later instead of doing the manipulation every time.\par
{\pntext\f1\'B7\tab}Saving isn't going to work.  After some fiddling and wrong steps, I got it to actually save.  It takes about 20 minutes.  I tried loading it and that also took a long time.  Long enough that I gave up.  There's definitely got to be a better way of doing this.\par
{\pntext\f1\'B7\tab}So the meat of my matrix is 6700 some columns where everything is 0-1.  I've been taking a string processing class and we've done a bunch of bitparallel algorithms, which is making me think of representing the rows with bit vectors.  That would reduce the size by a pretty large factor.  But then I have to do all the matrix operations on bit vector matricies.  I feel like I'm very likely not the first person to think of doing something like this, and since all the entries are 0-1 it may be quite possible to do all the matrix stuff with bitparallel operations.\par
{\pntext\f1\'B7\tab}After some googling, I haven't found anything about bitparallel matrices.  And implementing this stuff my self is kind of getting away from the real point of this excersize, so I think I'll leave this thread for now\par
{\pntext\f1\'B7\tab}So I looked up sparse matricies, because I'd heard the term a few times before, and since most of the columns in any row are zero, my matrix seems like it's pretty "sparse".  And indeed it fits the definition of a sparse matrix very well.  I found some scipy documentation on their sparse matrix representations, so I think I'll give those a look tomorrow.\par

\pard\sa200\sl276\slmult1\b 9/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 So sparse matricies.  I found some code showing that sklearn can handle them.  Which kind of makes sense.  As long as they implement their own operations it should be pretty seamless.  The sklearn code was using lasso, which I have only vaugely heard of before (my understanding is it's a scoring function for linear regression, but I could be wrong).  I think I'll try it first, the sklearn documentation on it says it's for sparse data.  And it should be easy enough to swap it out for something else later if I want.\par
{\pntext\f1\'B7\tab}So there are a bunch of different sparse matrix representations.  After reading the documentation on some of them, I think the best way to go for now is coo to build and csc to do the work.  That's what the sklearn example used.\par
{\pntext\f1\'B7\tab}Also just realized- I should put this on github.  I'm not sure I actually want to show it to anyone, but if I decide to later, putting it up now will look better, cause there'll be commit messages and so on.\par
{\pntext\f1\'B7\tab}Also what is this .idea folder that came with the kaggle dataset?  I should look that up at some point.\par
{\pntext\f1\'B7\tab}Having some difficulty actually getting the sparse matrices to build and save.\par

\pard\sa200\sl276\slmult1\b 13/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Scipy's mmwrite won't save bools.  Interesting.  I probably would've found that in the docs if I'd read them before using.\par
{\pntext\f1\'B7\tab}I'm not sure what a lot of these Lasso parameters mean.  Fit_intercept should be false if the data is already centred.  Well the data is 0-1.  Is that centred?  I'm going to put true, because it seems like if that's wrong, it'll cost efficiency, not correctness.\par
{\pntext\f1\'B7\tab}So this is a multiclass problem, and lasso is a form of linear regression, which is for binary classification.  In class we were taught we could deal with this in two ways- one vs all, or all vs all.  There are probably sklearn built-ins to do this, I'll do some googling.\par
{\pntext\f1\'B7\tab}So after googling, looks like there is a good support for multiclass of various types in general, but I may have to use logistic regression instead of lasso?  Maybe lasso is a subclass. \par
{\pntext\f1\'B7\tab}Theoretically this should work fine with lasso, but for simplicity's sake I'll go with logistic regression for now.  It apparently handles sparse input fine.\par

\pard\sa200\sl276\slmult1\b 14/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 So I finally took a look at that .idea folder- it's not from kaggle, it's pycharm's project settings.  I set git to ignore the workspace.xml part and keep committing the rest.\par
{\pntext\f1\'B7\tab}I've mapped cuisines to ints and added a basic sklearn setup for logisitic regression.  It's not working.  It seems to predict the same cuisine for every item (I did some step throughs of some parts of the code and y_predict was all the same number).\par
{\pntext\f1\'B7\tab}So looking at the internals of the logistic regression object after calling fit, it looks like the weight vectors are all zeros, but the intercepts are actual numbers.  I think I'll try telling it not to fit intercepts and see if that does anything useful.\par
{\pntext\f1\'B7\tab}Perhaps predicatably, that results in intercept 0 but coefficients also zero.  It's definitely time to step back and do more research, hopefully to properly understand what's going on with logistic regression and what the various parameters do.\par
{\pntext\f1\'B7\tab}So apparently somewhere along the way from loading in the json to loading the saved csr matrix, every entry ends up as zero.  This explains the logistic regression alg not working.  At least I have a direction to investigate now.\par

\pard\sa200\sl276\slmult1\b 29/12/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 Took a break for Christmas.  Very glad I kept a log of what I was doing.  Should definitely make this a habit.\par
{\pntext\f1\'B7\tab}So it's not the loading that's the problem.  The column calcultion is giving zeros everywhere.  I thought I made sure this was working before I moved on, but apparently I made a mistake.  Specifically, the line\line whats_cooking["ingredients"] == ingredient\line does not search the list in the ingredients column for the value of ingredient, which for some reason I believed it did.  Looking at alternatives now.\par
{\pntext\f1\'B7\tab}So doing this efficiently might be a bit more difficult than I originally imagined.  I could brute force it, but god that'll be so slow with a big table like this.  Found a stackoverflow question which gives a vectorized way, but it's a bit complicated.  Gonna break it into steps and print each bit and try and see if a) it works b) I can figure out why it works\par
{\pntext\f1\'B7\tab}So apply.(pd.series) on the column of lists spreads them out into columns, which is cool and potentially useful elsewhere as well.  Stack turns that into one column with an inner level that consists of all the rows.  Get dummies does what it says on the tin, but there's still two levels of index.  And the sum makes that one level.  I'm pretty clear \i what\i0  each step does, but still a bit unclear on the \i how\i0  part for the sum bit.\par
{\pntext\f1\'B7\tab}So that seems to have worked, but also the maximum value is 2 and not one.  Maybe an ingredient was listed twice in some recipe? \par
{\pntext\f1\'B7\tab}Found the recipies that had values of 2: (crumbled blue cheese, salt, large egg whites, vegetable oil, sugar, cooking spray, all-purpose flour, large eggs, 2%reduced-fat milk) and (2% reduced-fat milk, corn starch, eggs, maple syrup, sugar, margarine, vanilla extract).  No repeat ingredients (except maybe 'large egg whites' overlapping with 'large eggs' in the second, but there's no reason to expect non-exact matches).  There's the % sign in the 2% milk, but they're string objects, that really really shouldn't be causing any problems.  Not sure what's going on.  Which worries me.  \par
{\pntext\f1\'B7\tab}Running logisitic regression on 5000 entries gives me a ~50% correct prediction rate averaged over 3 cross validation folds.  Which is pretty exciting!  Actually better than random prediction!  Still a bit worried about the columns which have a value of 2, but I'm going to put a pin in that and come back to investigate later.\par
{\pntext\f1\'B7\tab}Also, it still works when doing a load instead of creating columns normally.  Which is excellent!\par

\pard\sa200\sl276\slmult1\b 2/1/2015\par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\b0 So running on the full data set with 10 cross validation folds gives me ~70% correct prediction.  Not entirely terrible, but I'm pretty sure I can do better.  First thing's first- I'm going to look into the issue with some cells getting values other than 0 or 1.  Hopefully I'll be able to figure out what's going on there.  And even if I can't, it's still likely a good idea to reduce all the 2s to 1s.\par
{\pntext\f1\'B7\tab}I may have found  the wrong recipies the last time.  I had been using argmax() and doing some math to calculate the relevant rows.  This time I did "(ing_col == 2).any(axis=1)" and used that as a row index.    Maybe I made a math error last time?  In particular, I think this is right because it gives a much more sensible answer.  I find only one row that has 2 valued cell(s), and it does have duplicate ingredients.  Going to check if I get similar results on a bigger portion of the data, and if I do, I'll just set everything greater than 1 to 1.\par
{\pntext\f1\'B7\tab}Also, apparently the matrix classes in numpy are sort of deprecated in favour of ndarrays?  I can see how having both is kind of annoying.  I need the sparse matrices, but I'll favor ndarrays over the regular ones.\par
{\pntext\f1\'B7\tab}So searching the whole data set, there are five recipies that show up as having values other than 0 or 1, and they all have duplicate ingredients.  So that answers that question.  As I said before, I'll just set everything above 1 to 1.\par
{\pntext\f1\'B7\tab}\par

\pard\sa200\sl276\slmult1\par
}
 